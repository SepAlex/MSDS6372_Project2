---
title: MSDS 6372 Project 2 - Using classification methods to determine an outcome
  of a telemarketing campaign.
author: "Swee K Chew, Rene Pineda, Volodymyr Orlov"
output:
  pdf_document: default
  html_document: null
  df_print: paged
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggpubr)
library(car)
```

## Introduction

While telemarketing might be considered as a cornerstone of modern advertising strategies by some companies, its role is highly questionable and sometimes it is viewed as a total waste of resources by others^[https://www.prospectresearch.co.uk/blog/telemarketing-still-effective/]. Here we attempt to analyze the effect of telemarketing on attracting new clients in a finance industry by looking at the success of telemarketing calls for selling bank long-term deposits recorded by a Portuguese retail bank. We apply multiple statistical methods and analyze outcomes of various models:

1. Logistic Regression models. We've built two models which can be used to predict our binary outcome and to interpret the effects of predictors.
1. We found that linear Discriminant Analysis model performed very poorly, in part due to the fact that only a handful of variables in the dataset are continuous.
1. Non-parametric model. This model has a slightly better performance than the Linear Regession models but is not easily interpretable. 

## Data Description

Our group focused on the Portuguese Bank Marketing data set^[https://archive.ics.uci.edu/ml/datasets/Bank+Marketing]. The data is a result of a direct marketing campaign performed by a Portuguese bank. The bank collected data from May 2008 to November 2010 and the data consist of 45,211 observations and 17 variables. The target response is a binary, categorical variable indicating whether a client subscribed to a term deposit or not. For a complete list of variables please refer to \autoref{table1}.

The count plot of the binary response variable of the original dataset in \autoref{fig1} suggests that the data is unbalanced. The number of 'no' responses is disproportionately higher than the 'yes' responses. 

We decided to train all our models on balanced and unbalanced samples taken from the original data to find out whether the results would be different.  
For the balanced sample, we took a random sample of 2500 'yes' and 2500 'no' responses. For the unbalanced sample, we simply randomly chose 5000 data points from our original dataset. 

For our test sample, we selected 1000 data points which do not overlap with either the balanced, or unbalanced samples.

All dataset turned out to be clean and no imputation was nesessary. 

## Exploratory Data Analysis

For our analysis, we have used all variables. We separate variables into categorical and continuous and examine each group separately.

First, we explore the original full dataset. By looking at the histograms and boxplots of the continuous variables in \autoref{fig2} and \autoref{fig3}, we find that _duration_, _balance_, _campaign_ , _pdays_ and _previous_ variables might have an impact on our binary response, while _age_ has no apparant effect on it. Also, the count plots of the categorical variables in \autoref{fig4} indicate that _day_ does not seem to have an impact on our dependent variable but the rest of the variables do. 

After we obtain the balanced training sample, we examine the frequency tables of the categorical variables and the summary statistics of the continuous variables for further exploratory analysis. 

\autoref{fig5} shows the counts and percentage frequencies of the categorical variables for each factor level by the response variable. It lets us see if a specific level or group of a factor has a higher or lower count than its counterparts that might contribute to the likelihood of subscribing a term deposit. 

The proportion of clients who subscribed to a term deposit seems to vary by job categories even for those with roughly the same sample size. For example, the proportion of subscribing to a term deposit is higher for clients who hold an administrative position and the proportion is lower for individuals who are self-employed. Thus, _job_ possibly has an effect on the likelihood of a client subscribing to a term deposit. 

Reviewing the frequency tables for the remaining categorical variables (Figure 6-13), it appears that all the variables could contribute, the proportions vary across the factor levels within each variable.

\autoref{fig14} displays the summary statistics for each continuous variables by the response variable _y_, which allows us to see if there are any differences in characteristics between clients who subscribe a term deposit and who do not.

Except the _age_ and _day_ variables, the mean of the remaining continuous variables varies between two response groups. We decided not to transform any of these variables in order to build a simplier model that is easy to interpret.

## Baseline model. Logistic Regression.

For our first two models, we fit logistic regression to balanced and unbalanced datasets. We estimate the performance of both models on the same test dataset to see if one model has a better predictive power than the other.

### Model Assumptions

In this section, we assess whether the modelâ€™s assumptions required for logistic regression analysis are met. 

We use the Hosmer and Lemeshow Goodness-of-Fit test with the null hypothesis that the fitted model is correct. The output p-value is a number between 0 and 1 with higher values indicating a better fit. The p-value we obtain from the test is <0.0001 (\autoref{fig15}), which is statistically significant and implies that the null hypothesis should be rejected. Paul D. Allison, however, shows in his paper that the Hosmer and Lemeshow test is not accurate enough to evaluate model's fit^[https://support.sas.com/resources/papers/proceedings14/1485-2014.pdf]. Moreover, since our goal is to measure the predictive power of a model and not the goodness of fit, we will proceed despite not meeting the assumption. 

We also look at the residual diagnostics for any potential leverage points. \autoref{fig16} displays some of the residual and influential plots from the SAS output. When we review all the influential plots, there seems to be no leverage points. 

Logistic regression also requires that there is little or no multicollinearity among the explanatory variables. The matrix scatter plot in \autoref{fig17} and the correlation matrix in \autoref{fig18} indicate that the continuous variables are not highly correlated with each other.

We assume that observations are independent of one another. Since the required assumptions have been addressed, we will proceed with model fitting. 

### Model Fit

First, the overall test is performed to test the null hypothesis that at least one coefficient is different from 0. Using the Likeliness Ratio test, we reject the null hypothesis at the significant level of 0.05 and conclude that the overall model is significan with the p-value <0.0001 (\autoref{fig19}). 

We then include all the main effects, both categorical and continuous variables, to see which predictors are significant. \autoref{fig20} (left) shows the output with all the main effects and their respective p-values. Based on the results, _education_, _default_, _age_, _balance_, _pdays_, and _previous_ are non-significant at the alpha level of 0.05. Thus, we remove these predictors and refit the model. The new output is shown in \autoref{fig20} (right).

### Parameter Interpretation

\autoref{fig21} displays the coefficient estimates for each factor level and \autoref{fig22} displays the odd ratio estimates and the confidence intervals for each level. Here is our interpretation of a subset of most interesting estimates.  

#### Job [categorical]:
The odds ratio of subscribing to a term-deposit for clients with unknown job title relative to clients who are entrepreneurs is 0.684 after accounting for other variables. The 95% confidence interval is [0.203,2.302]. In other words, the odds for someone with unknown job title to subscribe a term-deposit is 31.6% less than the odds for an entrepreneur. 

#### Marital [categorical]:
The odds ratio for a single client subscribing a term-deposit relative to a married client is 0.727 after accounting for other variables. The 95% confidence interval is [0.607,0.870]. In other words, the odds for a single client to subscribe a term-deposit is 27.3% less than the odds for a married client. 

#### Housing [categorical]:
The odds ratio of subscribing a term-deposit for clients with a housing loan relative to clients without a housing loan is 2.047 after accounting for other variables. The 95% confidence interval is [1.710,2.451]. In other words, the odds for someone with a housing loan to subscribe a term-deposit is 104.7% higher than the odds for someone without a housing loan. 

#### Loan [categorical]:
The odds ratio of subscribing a term-deposit for clients with a personal loan relative to clients without a housing loan is 1.581 after accounting for other variables. The 95% confidence interval is [1.239,2.019]. In other words, the odds for someone with a personal loan to subscribe a term-deposit is 58.1% higher than the odds for someone without a personal loan. 

#### Contact [categorical]:
The odds ratio of subscribing a term-deposit for clients whose contact communication type are unknown relative to clients who are communicated via cellular phone is 4.478 after accounting for other variables. The 95% confidence interval is [3.358,5.971]. In other words, the odds for someone who is contacted via an unknown method to subscribe a term-deposit is 347.8% higher than the odds for someone who is contacted via cellular. 

#### Month [categorical]:
The odds ratio of subscribing a term-deposit for clients who are last contacted in September relative to those who are last contacted in November is 0.080 after accounting for other variables. The 95% confidence interval is [0.042,0.156]. In other words, the odds for a client who is last contacted in September to subscribe a term-deposit is 92% less than the odds for a client who is last contacted in November.

#### Poutcome [categorical]:
The odds ratio of subscribing a term-deposit for clients with the unknown previous marketing campaign outcome relative to clients with the failure previous marketing campaign outcome is 1.566 after accounting for other variables. The 95% confidence interval is [1.233,1.989]. In other words, the odds for a client with the unknown previous marketing campaign outcome to subscribe a term-deposit is 56.6% higher than the odds for a client with the failure previous marketing campaign outcome. 

#### Day [Continuous]:
For every 1 unit increases in last contact day of the month, the odds of a client subscribing a term-deposit will increase by a multiplicative factor of 1.013 holding all other variables constant. The odds ratio (for a clients with the last contact day on the 15th compared to the 14th) is 1.013. The 95% confidence interval is [1.002,1.023].

#### Duration [Continuous]:
The odds of a client subscribing a term-deposit for a client is 1.006 times higher than a client whose last contact duration is 1 second less after accounting for other variables. The 95% confidence interval is [1.005,1.006]. In other words, for every minute increase in the duration of last contact, the odds of a client subscribing a term-deposit will increase by a multiplicative factor of 1.409 (exp[60*0.00572]) holding all other variables constant.

#### Campaign [Continuous]:
For every 1 unit increases in number of contacts performed during the campaign, the odds of a client subscribing a term-deposit will decrease by a multiplicative factor of 0.0894 holding all other variables constant. The odds ratio (10 contacts made compared to 11 contacts) is 0.0894. The 95% confidence interval is [0.859,0.929].

### Prediction Performance

Using the resulting model from the logistic regression, we examine the ROC curve on the balanced training dataset and also on the test dataset for the predictability power of the model. 

\autoref{fig23} shows the ROC curve of the training dataset (top) and the ROC curve on the test dataset (bottom). The area under the curve (AUC) is commonly used to assess the prediction performance of the logistics model, the closer it's to 1, the better the prediction is. The AUC based on the training data is 0.9096 and 0.9124 for the test data, which indicates that we did not overfit the model and the predicitibility power of the model is quite high. 

The classification tables in \autoref{fig24} can also be used to assess how well the model performs in classifying the dichotomous response variable. The accuracy is measured by its sensitivity (the ability to predict an event correctly) and specificity (the ability to predict a nonevent correctly). At the probability level of 0.5, the model can correctly classify 81.1% of the event and 84.2% of the non-event, with an overall rate of 82.7% on the training data. For the test data, the sensitivity drops to 29.3%, with more false positive predictions of 54.1% of the event. However, the specificity and the overall accuracy increase to 97.9% and 93.9% respectively.

It could be the results of having very low counts of 'yes' responses in the test dataset and setting the probability cutpoint to 0.5. The test data contains only 58 'yes' records out of 1000 observations. We could adjust the cutpoint to predict more events correctly but at the expense of more false predictions.


### Using Unblanced Training Dataset

The analyses we have done so far are based on the balanced training dataset. We would like to find out if we will get a different logistic regression model if the training dataset is unbalanced, thus we repeat the analyses using the unbalanced training dataset. 

Due to the disproportionate sample size ratio of approximately 1:7 (yes:no), it's difficult to determine whether any of the variables have an influence on the likelihood of a client subscribing a term deposit just by looking at the frequency tables and the summary statistics table. Thus, we simply include all the variables in the model and let it decide which predictors are significant.

At the significant level of 0.05, _default_, _age_, _balance_, _pdays_, and _previous_ are non-significant (\autoref{fig25} (left)). The _education_ variable is statistically significant here, whereas it was shown non-significant in the prior model under the balanced dataset. We then remove the non-significant predictors and refit the model, the output is shown in \autoref{fig25} (right).

Using the resulting model that is built with the unbalanced dataset, we examine the ROC curve of the training dataset and also on the same test dataset to determine the predictability power of the model. 

\autoref{fig26} (top) illustrates the ROC curve on the traning dataset and \autoref{fig26} (bottom) displays the ROC curve on the test dataset. The AUC is 0.9012 for the model based on the training data and 0.9054 for the test data. The values are slighly lower than those that are obtained from the balanced model respectively. 

The classification table in \autoref{fig27} (top) displays the sensitivity and the specificity of the model. At the probability level of 0.5, the model can correctly classify 31.9% of the event and 97.2% of the non-event, with an overall rate of 89.2% on the training data. For the test data, the sensitivity drops to 27.6%, with more false positive predictions of 54.3% of the event. However, the specificity and the overall accuracy increase to 93.9% and 98.0% respectively.

Compared to the prior model with the balanced training data, the sensitivity is much lower and the specificity is higher, which makes sense since the latter model is built based on the disproportionate ratio of 'no' and 'yes' responses, having a much higher observations of 'no' than 'yes'. Thus, the model can more accurately classify the nonevents resulting in higher specificity. On the other hand, the sensitivity is low due to the small number of 'yes' records in the training dataset. Thus, there is not enough information for the model to correctly classify the event.

Since the prediction accuracy is better with the balanced training data, we will only use the balanced data in fitting additional models and for further analyses.  

## Additional Models

### Logistic Regression model (LRM) with transformed variables

#### Motivation: 
The transformation of varaibles for this objective is focused on creating categories for the continuous variables. This responds to two reasons:

- A logistic regression model will typically assign a weight to a continuous feature, and always think that every feature is either positively or negatively related to the outcome variable. However, for some variables (for example _balance_) the feature might be positively related with the outcome for one range of values, and negatively related for other ranges. Discretization of continuous features is simple but is a useful way to include additional information that might solve this problem, and we will pay special attention to those variables that were deemed as non-significant by the baseline model. 

- In other instances, the creation of categorical variables responds to the need of highlighting information that is hidden or not explicit in the continuous variable. For example, the variable _pdays_ contains information about whether a client was previously contacted or not, but because this is indicated by assigning the variable a value of -1, this information can't be picked up by a regression model that uses the continuous variable.

The upper and lower limits for the categories are based on our analysis of the distribution of the features.

##### Age:
Create three categories: Adult (up to 35 yo), Middle aged (36 to 60 yo), and Elderly (65 yo and more)

##### Balance:
Create categories for negative balance, zero balance, and 5 levels for positive balance: $0 to $100, $101 to $500, $501 to $2,000, $2,0001 to $10,000, and more than $10,000. 

##### Campaign:
Create categories for clients that were contacted only once or twice and for those who were contacted more than two times during the campaign.

##### pdays:
Add variable to indicate whether a client was previously contacted or not. Additionally, convert days to months and create three categories depending on how much time had passed since the client was last contacted.

##### previous:
Create a category for clients that were previously contacted only once or twice and another category for those who were contacted more times.

#### Model Building

To create the logistic regression model, we use the Glmnet package in R, which fits a generalized linear model via penalized maximum likelihood. We perform a cross-validation fit, which is shown in \autoref{fig28}. The potential model that minimizes the misclassification error includes between 23 to 42 features. 

We can generate a list of the coefficients for the value of lambda that gives minimum mean cross-validated error, which is part of the output of the Glmnet package. By examining these coefficients, we can tell that the new categories we created for the _balance_, _campaign_, and _pdays_ variables are not selected by the model, similar to the results of the first model we produced. However, it is interesting to notice that the selection process picks up the negative balance, zero balance, balance between $2,000 and $10,000, and balance greater than $10,000 as important, indicating that splitting _balance_ into categorical variables is useful.

#### Prediction Performance

Regarding predictive accuracy, the model with the new categorical variables shows a similiar performance compared to the first model developed on the balanced dataset. The model achieves an accuracy of 82.4% for the training set and 85.9% for the test set. This new model shows the same performance when we measure the AUC indicator, which is the same than the first logistic regression model (0.912). The ROC curve of the new model is shown in \autoref{fig29}.

### Linear Discriminant Analysis model (LDA)

Next, we develop a LDA model using only the continuous predictors. This poses a serious challenge because only 6 variables in the dataset are continuous. Additionally, the logistic regression models we have developed indicate that out of these 6 variables, four are non-significant (_age_, _balance_, _pdays_ and _previous_) and their predictive power is low, although one continuous variable (_duration_) is perhaps the strongest predictor of all.

Likely due to these limitations, the LDA model performs poorly. Examining the confusion matrix, we conclude that the model has an accuracy of only 74.1% on the training set and 64.7% on the testing set, much lower than the logistic regression models. Based on the ROC curve in \autoref{fig30}, the AUC score of 0.805 is also much lower compared to the logistic regression models. 

The conclusion from the LDA model is that the two categories in the outcome binomial variable are not clearly separable on the continuous features. Thus, the LDA model is not an appropriate method for this specific dataset. 

### Non-parametric model. Random Forest (RF)

The third additional model we developed is based on the Random Forest package in R. To make this model works smoothly, we decided to modify the original dataset as follows: i) Create dummy variables for all the categorical predictors, ii) Create an additional dummy variable that indicates whether the client was previously contacted or not. 

Reviewing the prediction accuracy and the AUC of the random forest model in ROC curve (\autoref{fig31}), it performs significantly better than LDA model, and marginally better than the logistic regression models. The accuracy of the model is 85.4% on the training set and 82.6% on the testing set. This model is especially good at predicting the 'yes' cases (clients who will actually sign up for the term deposit), with a sensitivity of around 88% on both the training and testing sets. However, this model performs relatively poorly on the overall accuracy on the test set, which is brought down by a poor sensitivity rate. This might be due to the fact that the proportion of 'yes' in the test set was very low.

Other disadvantages of this model are as follows:

- The model is not easily interpretable: we can have an idea about which factors impact the outcome by using the "importance" function, which displays the mean Gini gain produced by the X's over all trees, and the mean decrease in classification accuracy after permuting X's over all trees. Based on this, we can assess which variables are more important on a relative scale, but there is no absolute measure of this and the model is not interpretable.

- May require some work to tune the model to the data: the Random Forest has two main tuning parameters: the number of trees created (ntrees, default 500), and the number of features that are randomly selected at each split (mtry, default = the sq root of the number of features). The model we ran has the default parameters. We attempt to tune the model by increasing and decreasing the parameters, however, we do not obtain a better overall performance. 

## Comparison of all models and Conclusion

\autoref{table2} shows a comparison of the performance of the four models that are fitted on the balanced data, along with four metrics.

Based on this information, we can conclude that the random forest model is suitable if the goal is to obtain the model with high predictive power. The limitation of low interpretability of the results can be overcome by understanding how different factors affect the outcome, as explained in the Exploratory Data Analysis section. However, if the interpretability is crucial, one could use the logistic regression models to better understand how individual factor levels can influence the likelihood a client subscribing to a term deposit.

## Code

All codes used to generate models, plots and report related to this work can be found in [https://github.com/VolodymyrOrlov/MSDS6372_Project2](https://github.com/VolodymyrOrlov/MSDS6372_Project2)

## Tables and Figures

\begin{table}[h]
\centering
 \begin{tabular}{|p{0.2\linewidth}|p{0.2\linewidth}|p{0.4\linewidth}|}
 \hline
 Variable Name & Variable Type & Description  \\ \hline
 job  &  categorical  & type of job ('admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown') \\ 
 marital  & categorical  & marital status ('divorced', 'married', 'single', 'unknown')  \\ 
 education  & categorical  & 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown'  \\ 
 default  & categorical  & has credit in default? ('no', 'yes', 'unknown')  \\ 
 housing  & categorical  & has housing loan? ('no', 'yes', 'unknown')  \\ 
 loan  & categorical  & has personal loan? ('no', 'yes', 'unknown')  \\ 
 contact  & categorical  & contact communication type ('cellular', 'telephone')  \\ 
 month  & categorical  & last contact month of year ('jan', 'feb', 'mar', ..., 'nov', 'dec')  \\ 
 poutcome  & categorical  & outcome of the previous marketing campaign ('failure', 'nonexistent', 'success')  \\ 
 age  & continuous  & age of the contact   \\ 
 balance  & continuous  & average yearly balance, in euros  \\ 
 day  & continuous  & last contact day  \\ 
 duration  & continuous  & last contact duration, in seconds  \\ 
 campaign  & continuous  & number of contacts performed during this campaign and for this client  \\ 
 pdays  & continuous  & number of days that passed by after the client was last contacted from a previous campaign  \\ 
 previous  & continuous  & number of contacts performed before this campaign and for this client  \\
 \hline
 \end{tabular}
 \caption{List of variables.}
 \label{table1}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Summary       & \multicolumn{3}{l|}{Training Set Statistics} & \multicolumn{3}{l|}{Test Set Statistics} &       \\ \hline
Model         & Accuracy    & Sensitivity    & Specificity   & Accuracy   & Sensitivity  & Specificity  & AUC   \\ \hline
LR model 1    & 0.827       & 0.811          & 0.842         & 0.939      & 0.293        &  0.979       & 0.912 \\ \hline
LR model 2    & 0.824       & 0.796          & 0.852         & 0.859      & 0.863        & 0.793        & 0.912 \\ \hline
LDA           & 0.741       & 0.658          & 0.824         & 0.647      & 0.827        & 0.636        & 0.805 \\ \hline
Random Forest & 0.854       & 0.884          & 0.825         & 0.826      & 0.879        & 0.825        & 0.923 \\ \hline
\end{tabular}
\caption{Performance characteristics of all models.}
 \label{table2}
\end{table}


\begin{figure}[h]
  \centering
    \includegraphics[width=0.8\textwidth]{images/fig1.png}
  \caption{Count plot of the response variable.}
  \label{fig1}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=1.0\textwidth]{images/fig2.png}
  \caption{Histograms of continuous variables.}
  \label{fig2}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=1.0\textwidth]{images/fig3.png}
  \caption{Boxplots of continuous variables.}
  \label{fig3}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=1.0\textwidth]{images/fig4.png}
  \caption{Count plots of categorical variables.}
  \label{fig4}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig5_job.png}
  \caption{Frequency table of job type by the response variable.}
  \label{fig5}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig6_marital.png}
  \caption{Frequency table of marital status by the response variable.}
  \label{fig6}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig7_educ.png}
  \caption{Frequency table of education level by the response variable.}
  \label{fig7}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig8_default.png}
  \caption{Frequency table of default (has credit or not) by the response variable.}
  \label{fig8}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig9_housing.png}
  \caption{Frequency table of housing loan by the response variable.}
  \label{fig9}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig10_loan.png}
  \caption{Frequency table of personal loan by the response variable.}
  \label{fig10}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig11_contact.png}
  \caption{Frequency table of contact type by the response variable.}
  \label{fig11}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig12_month.png}
  \caption{Frequency table of last contact month by the response variable.}
  \label{fig12}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig13_poutcome.png}
  \caption{Frequency table of the outcome of the previous marketing campaign by the response variable.}
  \label{fig13}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.8\textwidth]{images/fig14_summary.png}
  \caption{Summary statistics of the continuous variables by the response variable.}
  \label{fig14}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig15_GOF.png}
  \caption{Hosmer and Lemeshow Goodness-of-Fit Test Result.}
  \label{fig15}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.4\textwidth]{images/fig16_residual_1.png}
    \includegraphics[width=0.4\textwidth]{images/fig16_residual_2.png}
    \includegraphics[width=0.4\textwidth]{images/fig16_residual_3.png}
    \includegraphics[width=0.4\textwidth]{images/fig16_residual_4.png}
  \caption{Residual and influential diagnostics plots.}
  \label{fig16}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.6\textwidth]{images/fig17_scatter.png}
  \caption{Matrix scatterplot of the continuous explanatory variables.}
  \label{fig17}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.8\textwidth]{images/fig18_corr.png}
  \caption{Correlation matrix of the continuous explanatory variables.}
  \label{fig18}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig19_overall_test.png}
  \caption{Overall test of Logistic Regression.}
  \label{fig19}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig20a_typeIII.png} 
    \includegraphics[width=0.5\textwidth]{images/fig20b_typeIII.png}
  \caption{Type 3 analysis of effects with all the predictors (left) and with only the predictors that are significant (right) using the balanced dataset.}
  \label{fig20}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.8\textwidth]{images/fig21_coefficients.png} 
  \caption{Tables of Coefficient estimates.}
  \label{fig21}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.8\textwidth]{images/fig22_odds.png}
  \caption{Tables of Odds Ratio estimates and confidence intervals.}
  \label{fig22}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig23a_ROC.png} 
    \includegraphics[width=0.5\textwidth]{images/fig23b_ROC.png}
  \caption{ROC curves for the balanced training dataset and the test dataset.}
  \label{fig23}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.6\textwidth]{images/fig24a_ctable.png} 
    \includegraphics[width=0.6\textwidth]{images/fig24b_ctable.png} 
  \caption{The classification table based on the balanced training (top) and test (bottom) datasets.}
  \label{fig24}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig25a_typeIII.png} 
    \includegraphics[width=0.5\textwidth]{images/fig25b_typeIII.png}
  \caption{Type 3 analysis of effects with all the predictors (left) and with only the predictors that are significant (right) using the unbalanced training dataset.}
  \label{fig25}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/fig26a_ROC.png} 
    \includegraphics[width=0.5\textwidth]{images/fig26b_ROC.png}
  \caption{ROC curves for the unbalanced training dataset and the test dataset.}
  \label{fig26}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.6\textwidth]{images/fig27a_ctable.png} 
    \includegraphics[width=0.6\textwidth]{images/fig27b_ctable.png} 
  \caption{The classification table based on the unbalanced training (top) and test (bottom) datasets.}
  \label{fig27}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/LRM-1.png} 
  \caption{Misclassification error for the second LR model.}
  \label{fig28}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/LRM-2.png} 
  \caption{ROC curve for the second LR model.}
  \label{fig29}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/LDA-1.png} 
  \caption{ROC curve for the LDA model.}
  \label{fig30}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/RF-1.png} 
  \caption{ROC curve for the Random Forest model.}
  \label{fig31}
\end{figure}



